{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd1cf21c",
   "metadata": {},
   "source": [
    "# Part 3: Mining Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0506a501",
   "metadata": {},
   "source": [
    "This part uses the Coronavirus Tweets NLP data set from Kaggle https://www.kaggle.com/ datatattle/covid-19-nlp-text-classification to predict the sentiment of Tweets relevant to Covid. The data set (Corona NLP test.csv file) contains 6 attributes:\n",
    "\n",
    "    UserName: Anonymized attribute\n",
    "    ScreenName: Anonymized attribute\n",
    "    Location: Location of the person having made the tweet\n",
    "    TweetAt: Date\n",
    "    OriginalTweet: Textual content of the tweet\n",
    "    Sentiment: Emotion of the tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00c66f4",
   "metadata": {},
   "source": [
    "Because this is a quite big data set, existing vectorized (pandas) functions have been particularly useful to effectively perform the various tasks with a typical personal computer. In this way, you will be able to run the code in few seconds. Otherwise, running the code might require a significant amount of time, e.g. in the case where for-loops are used for accessing all elements of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e51cf22",
   "metadata": {},
   "source": [
    "You need to install the nltk package to run the following code. You can do so by running the following command in your terminal:\n",
    "\n",
    "    pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4187cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import re\n",
    "import urllib\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "# Return a pandas dataframe containing the data set.\n",
    "# Specify a 'latin-1' encoding when reading the data.\n",
    "# data_file will be populated with the string 'wholesale_customers.csv'.\n",
    "def read_csv_3(data_file):\n",
    "    # read csv file using Pandas\n",
    "    df = pd.read_csv(data_file, encoding='latin-1')\n",
    "    return df\n",
    "\n",
    "# Return a list with the possible sentiments that a tweet might have.\n",
    "def get_sentiments(df):\n",
    "    return df['Sentiment'].unique()\n",
    "\n",
    "# Return a string containing the second most popular sentiment among the tweets.\n",
    "def second_most_popular_sentiment(df):\n",
    "    return df['Sentiment'].value_counts().index.tolist()[1]\n",
    "\n",
    "# Return the date (string as it appears in the data) with the greatest number of extremely positive tweets.\n",
    "def date_most_popular_tweets(df):\n",
    "    new_df = df[df['Sentiment'] == 'Extremely Positive']\n",
    "    return \"{}\".format(new_df['TweetAt'].mode().iloc[0])\n",
    "\n",
    "# Modify the dataframe df by converting all tweets to lower case. \n",
    "def lower_case(df):\n",
    "    df['OriginalTweet'] = [str(i).lower() for i in df['OriginalTweet']]\n",
    "    return df\n",
    "\n",
    "# Modify the dataframe df by replacing each characters which is not alphabetic or whitespace with a whitespace.\n",
    "def remove_non_alphabetic_chars(df):\n",
    "#     df['OriginalTweet'] = [re.sub('[^a-zA-Z]', ' ', str(i)) for i in df['OriginalTweet']]\n",
    "    df[\"OriginalTweet\"] = df[\"OriginalTweet\"].str.replace('[^a-zA-Z]', ' ', regex=True)\n",
    "    return df\n",
    "\n",
    "# Modify the dataframe df with tweets after removing characters which are not alphabetic or whitespaces.\n",
    "def remove_multiple_consecutive_whitespaces(df):\n",
    "#     df['OriginalTweet'] = [re.sub(' +', ' ', str(i)) for i in df['OriginalTweet']]\n",
    "#     df['OriginalTweet'] = [str(i).str.replace(' +', ' ', regex=True) for i in df['OriginalTweet']]\n",
    "    df[\"OriginalTweet\"] = df[\"OriginalTweet\"].str.replace(' +', ' ', regex=True)\n",
    "    return df\n",
    "\n",
    "# Given a dataframe where each tweet is one string with words separated by single whitespaces,\n",
    "# tokenize every tweet by converting it into a list of words (strings).\n",
    "def tokenize(df):\n",
    "    df['OriginalTweet'] = [str(i).split() for i in df['OriginalTweet']]\n",
    "    return df\n",
    "\n",
    "# Given dataframe tdf with the tweets tokenized, return the number of words in all tweets including repetitions.\n",
    "def count_words_with_repetitions(tdf):\n",
    "    return sum([len(i) for i in tdf['OriginalTweet']])\n",
    "\n",
    "# Given dataframe tdf with the tweets tokenized, return the number of distinct words in all tweets.\n",
    "def count_words_without_repetitions(tdf):\n",
    "    tweets = []\n",
    "    for i in tdf['OriginalTweet']:\n",
    "        tweets.extend(i)\n",
    "    unique_words = set(tweets)\n",
    "    num_of_unique_words = len(unique_words)\n",
    "    return num_of_unique_words\n",
    "\n",
    "# Given dataframe tdf with the tweets tokenized, return a list with the k distinct words that are most frequent in the tweets.\n",
    "def frequent_words(tdf,k):\n",
    "    freq_words = []\n",
    "    [freq_words.extend(i) for i in tdf['OriginalTweet']]\n",
    "    counter = Counter(freq_words)\n",
    "    return [item[0] for item in counter.most_common(k)]\n",
    "\n",
    "# Given dataframe tdf with the tweets tokenized, remove stop words and words with <=2 characters from each tweet.\n",
    "# The function should download the list of stop words via:\n",
    "# https://raw.githubusercontent.com/fozziethebeat/S-Space/master/data/english-stop-words-large.txt\n",
    "def remove_stop_words(tdf):\n",
    "    data = urllib.request.urlopen('https://raw.githubusercontent.com/fozziethebeat/S-Space/master/data/english-stop-words-large.txt')\n",
    "    all_words = []\n",
    "    for line in data: # files are iterable\n",
    "        line = line.decode(\"utf-8\")\n",
    "        line = line.replace('\\n', '')\n",
    "        all_words.append(line)\n",
    "\n",
    "    tdf['OriginalTweet'] = tdf['OriginalTweet'].apply(lambda row: [word for word in row if word not in all_words and len(word) > 2])\n",
    "    return tdf\n",
    "\n",
    "# Given dataframe tdf with the tweets tokenized, reduce each word in every tweet to its stem.\n",
    "def stemming(tdf):\n",
    "    tdf['OriginalTweet'] = tdf['OriginalTweet'].apply(lambda row: [PorterStemmer().stem(word) for word in row])\n",
    "#     tdf['OriginalTweet'] = remove_stop_words(tdf)['OriginalTweet']\n",
    "#     for i in tdf['OriginalTweet']:\n",
    "#         tdf['OriginalTweet'] = [PorterStemmer().stem(y) for y in i]\n",
    "#         tdf['OriginalTweet'] = [PorterStemmer().stem(y) for y in tdf['OriginalTweet']]\n",
    "    return tdf\n",
    "\n",
    "# Given a pandas dataframe df with the original coronavirus_tweets.csv data set,\n",
    "# build a Multinomial Naive Bayes classifier. \n",
    "# Return predicted sentiments (e.g. 'Neutral', 'Positive') for the training set\n",
    "# as a 1d array (numpy.ndarray). \n",
    "def mnb_predict(df):\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(4,6))\n",
    "    y = df['Sentiment']\n",
    "    stemming_tokenize = stemming(tokenize(df))\n",
    "    data = stemming_tokenize['OriginalTweet'].apply(lambda i: \" \".join(i))\n",
    "    x = vectorizer.fit_transform(data)\n",
    "    clf = nb.MultinomialNB()\n",
    "    clf.fit(x, y)\n",
    "    y_hat = clf.predict(x)\n",
    "    return y_hat\n",
    "\n",
    "# Given a 1d array (numpy.ndarray) y_pred with predicted labels (e.g. 'Neutral', 'Positive') \n",
    "# by a classifier and another 1d array y_true with the true labels, \n",
    "# return the classification accuracy rounded in the 3rd decimal digit.\n",
    "def mnb_accuracy(y_pred,y_true):\n",
    "    count = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_pred[i] == y_true[i]:\n",
    "            count += 1\n",
    "    score = round(count / len(y_true), 3)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5335b912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second_most_popular_sentiment: Negative\n",
      "get_sentiments: ['Neutral' 'Positive' 'Extremely Negative' 'Negative' 'Extremely Positive']\n",
      "date_most_popular_tweets: 25-03-2020\n",
      "count_words_with_repetitions: 1255301\n",
      "count_words_without_repetitions: 136386\n",
      "frequent_words: ['the', 'to', 'and', 'of', 'a', 'in', 'for', '#coronavirus', 'is', 'are']\n",
      "remove_stop_words:        UserName  ScreenName                      Location     TweetAt  \\\n",
      "0          3799       48751                        London  16-03-2020   \n",
      "1          3800       48752                            UK  16-03-2020   \n",
      "2          3801       48753                     Vagabonds  16-03-2020   \n",
      "3          3802       48754                           NaN  16-03-2020   \n",
      "4          3803       48755                           NaN  16-03-2020   \n",
      "...         ...         ...                           ...         ...   \n",
      "41152     44951       89903  Wellington City, New Zealand  14-04-2020   \n",
      "41153     44952       89904                           NaN  14-04-2020   \n",
      "41154     44953       89905                           NaN  14-04-2020   \n",
      "41155     44954       89906                           NaN  14-04-2020   \n",
      "41156     44955       89907  i love you so much || he/him  14-04-2020   \n",
      "\n",
      "                                           OriginalTweet           Sentiment  \n",
      "0      [@MeNyrbie, @Phil_Gahan, @Chrisitv, https://t....             Neutral  \n",
      "1      [advice, Talk, neighbours, family, exchange, p...            Positive  \n",
      "2      [Coronavirus, Australia:, Woolworths, give, el...            Positive  \n",
      "3      [food, stock, empty..., PLEASE,, panic,, THERE...            Positive  \n",
      "4      [Me,, ready, supermarket, #COVID19, outbreak.,...  Extremely Negative  \n",
      "...                                                  ...                 ...  \n",
      "41152  [Airline, pilots, offering, stock, supermarket...             Neutral  \n",
      "41153  [Response, complaint, provided, citing, COVID-...  Extremely Negative  \n",
      "41154  [You, itÂs, tough, @KameronWilds, rationing, ...            Positive  \n",
      "41155  [wrong, smell, hand, sanitizer, starting, turn...             Neutral  \n",
      "41156  [@TartiiCat, Well, new/used, Rift, $700.00, Am...            Negative  \n",
      "\n",
      "[41157 rows x 6 columns]\n",
      "stemming:        UserName  ScreenName                      Location     TweetAt  \\\n",
      "0          3799       48751                        London  16-03-2020   \n",
      "1          3800       48752                            UK  16-03-2020   \n",
      "2          3801       48753                     Vagabonds  16-03-2020   \n",
      "3          3802       48754                           NaN  16-03-2020   \n",
      "4          3803       48755                           NaN  16-03-2020   \n",
      "...         ...         ...                           ...         ...   \n",
      "41152     44951       89903  Wellington City, New Zealand  14-04-2020   \n",
      "41153     44952       89904                           NaN  14-04-2020   \n",
      "41154     44953       89905                           NaN  14-04-2020   \n",
      "41155     44954       89906                           NaN  14-04-2020   \n",
      "41156     44955       89907  i love you so much || he/him  14-04-2020   \n",
      "\n",
      "                                           OriginalTweet           Sentiment  \n",
      "0      [@menyrbi, @phil_gahan, @chrisitv, https://t.c...             Neutral  \n",
      "1      [advic, talk, neighbour, famili, exchang, phon...            Positive  \n",
      "2      [coronaviru, australia:, woolworth, give, elde...            Positive  \n",
      "3      [food, stock, empty..., please,, panic,, there...            Positive  \n",
      "4      [me,, readi, supermarket, #covid19, outbreak.,...  Extremely Negative  \n",
      "...                                                  ...                 ...  \n",
      "41152  [airlin, pilot, offer, stock, supermarket, she...             Neutral  \n",
      "41153  [respons, complaint, provid, cite, covid-19, r...  Extremely Negative  \n",
      "41154  [you, itâ, tough, @kameronwild, ration, toile...            Positive  \n",
      "41155  [wrong, smell, hand, sanit, start, turn, on?, ...             Neutral  \n",
      "41156  [@tartiicat, well, new/us, rift, $700.00, amaz...            Negative  \n",
      "\n",
      "[41157 rows x 6 columns]\n",
      "y_pred: ['Neutral' 'Positive' 'Positive' ... 'Positive' 'Neutral' 'Negative']\n",
      "mnb_accuracy: 0.994\n"
     ]
    }
   ],
   "source": [
    "# Running all functions\n",
    "# feel free to comment out lines you don't want to run\n",
    "\n",
    "df = read_csv_3('data/coronavirus_tweets.csv')\n",
    "# print(df)\n",
    "print(\"second_most_popular_sentiment:\", second_most_popular_sentiment(df))\n",
    "print(\"get_sentiments:\", get_sentiments(df))\n",
    "print(\"date_most_popular_tweets:\", date_most_popular_tweets(df))\n",
    "# print(\"lower_case:\", lower_case(df))\n",
    "# print(\"remove_non_alphabetic_chars:\", remove_non_alphabetic_chars(df))\n",
    "# print(\"remove_multiple_consecutive_whitespaces:\", remove_multiple_consecutive_whitespaces(df))\n",
    "tdf=tokenize(df)\n",
    "print(\"count_words_with_repetitions:\", count_words_with_repetitions(tdf))\n",
    "print(\"count_words_without_repetitions:\", count_words_without_repetitions(tdf))\n",
    "print(\"frequent_words:\", frequent_words(tdf,10))\n",
    "print(\"remove_stop_words:\", remove_stop_words(tdf))\n",
    "\n",
    "print(\"stemming:\", stemming(tdf))\n",
    "y_pred = mnb_predict(stemming(tokenize(df)))\n",
    "print(\"y_pred:\", y_pred)\n",
    "print(\"mnb_accuracy:\", mnb_accuracy(y_pred, df['Sentiment'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79926c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
